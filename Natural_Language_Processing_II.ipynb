{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d770e96-327c-478d-b610-a2a07f0f84d8",
      "metadata": {
        "id": "8d770e96-327c-478d-b610-a2a07f0f84d8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425fb7bc-2e16-4dfd-bfa8-14f55d45113b",
      "metadata": {
        "id": "425fb7bc-2e16-4dfd-bfa8-14f55d45113b"
      },
      "source": [
        "**Ngrams**\n",
        "\n",
        "An n-gram is a sequence of n words: a 2-gram (which weâ€™ll call bigram) is a two-word sequence of words like \"please turn\", \"turn your\", or \"your homework\", and a 3-gram (a trigram) is a three-word sequence of words like \"please turn your\", or \"turn your homework\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd04583-4a60-4ce7-8929-b7fef257a06d",
      "metadata": {
        "id": "9fd04583-4a60-4ce7-8929-b7fef257a06d"
      },
      "outputs": [],
      "source": [
        "sentence = \"An n-gram is a sequence of n adjacent symbols in particular order.\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8afd6397-29d4-452f-b0d0-786f88a0df6d",
      "metadata": {
        "id": "8afd6397-29d4-452f-b0d0-786f88a0df6d"
      },
      "outputs": [],
      "source": [
        "# Creating bigrams\n",
        "\n",
        "from nltk.util import bigrams\n",
        "\n",
        "bgrams = list(bigrams(tokens))\n",
        "print(bgrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3368900a-2270-4bea-b669-f6a5c1304098",
      "metadata": {
        "id": "3368900a-2270-4bea-b669-f6a5c1304098"
      },
      "outputs": [],
      "source": [
        "# start and end padding\n",
        "\n",
        "from nltk.util import pad_sequence\n",
        "\n",
        "padded_tokens = list(pad_sequence(\n",
        "    tokens,\n",
        "    pad_left=True,\n",
        "    left_pad_symbol=\"<s>\",\n",
        "    pad_right=True,\n",
        "    right_pad_symbol=\"</s>\",\n",
        "    n=2\n",
        "))\n",
        "print(padded_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7bb0665-8f0d-4342-956c-a35a22e4f9b0",
      "metadata": {
        "id": "e7bb0665-8f0d-4342-956c-a35a22e4f9b0"
      },
      "outputs": [],
      "source": [
        "# alternate shorthand\n",
        "\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "\n",
        "padded_tokens = list(pad_both_ends(tokens, n=2))\n",
        "print(padded_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13825e74-1e9b-4925-b587-bcd39d94f5e9",
      "metadata": {
        "id": "13825e74-1e9b-4925-b587-bcd39d94f5e9"
      },
      "outputs": [],
      "source": [
        "# padded bigrams\n",
        "\n",
        "padded_bgrams = list(bigrams(padded_tokens))\n",
        "print(padded_bgrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcc43c4b-6717-49d2-932f-012de312fd33",
      "metadata": {
        "id": "fcc43c4b-6717-49d2-932f-012de312fd33"
      },
      "outputs": [],
      "source": [
        "# trigrams\n",
        "\n",
        "from nltk.util import trigrams\n",
        "\n",
        "padded_tgrams = list(trigrams(padded_tokens))\n",
        "print(padded_tgrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b322f52-dcd3-4841-a11f-59d74287a06c",
      "metadata": {
        "id": "3b322f52-dcd3-4841-a11f-59d74287a06c"
      },
      "outputs": [],
      "source": [
        "# ngrams\n",
        "\n",
        "from nltk.util import ngrams\n",
        "\n",
        "padded_5grams = list(ngrams(padded_tokens, n=5))\n",
        "print(padded_5grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd17dd37-246e-470b-9736-1662d5e8762a",
      "metadata": {
        "id": "fd17dd37-246e-470b-9736-1662d5e8762a"
      },
      "outputs": [],
      "source": [
        "# everygrams - ngrams for every n until max_len\n",
        "\n",
        "from nltk.util import everygrams\n",
        "\n",
        "everygrams3 = list(everygrams(padded_tokens, max_len=3))\n",
        "print(everygrams3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e05f3b-7021-4990-92f6-cb5173305428",
      "metadata": {
        "id": "f7e05f3b-7021-4990-92f6-cb5173305428"
      },
      "source": [
        "**Ngrams for Language Modelling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "297606eb-799e-4569-825e-d4ac241cebaf",
      "metadata": {
        "id": "297606eb-799e-4569-825e-d4ac241cebaf"
      },
      "outputs": [],
      "source": [
        "text = \"An n-gram is a sequence of n adjacent symbols in particular order. The symbols may be n adjacent letters (including punctuation marks and blanks), syllables, or rarely whole words found in a language dataset; or adjacent phonemes extracted from a speech-recording dataset, or adjacent base pairs extracted from a genome. They are collected from a text corpus or speech corpus. If Latin numerical prefixes are used, then n-gram of size 1 is called a \\\"unigram\\\", size 2 a \\\"bigram\\\" (or, less commonly, a \\\"digram\\\") etc. If, instead of the Latin ones, the English cardinal numbers are furtherly used, then they are called \\\"four-gram\\\", \\\"five-gram\\\", etc.\"\n",
        "N = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5777a29-adfe-436a-9a45-86cd9d017cc1",
      "metadata": {
        "id": "f5777a29-adfe-436a-9a45-86cd9d017cc1"
      },
      "outputs": [],
      "source": [
        "# divide text into sentences\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sentences = sent_tokenize(text.lower())\n",
        "sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e092c744-e7fc-431c-bbe3-1cb35cbcd3f7",
      "metadata": {
        "id": "e092c744-e7fc-431c-bbe3-1cb35cbcd3f7"
      },
      "outputs": [],
      "source": [
        "# divide each sentence into words\n",
        "\n",
        "tokens_list = [word_tokenize(sentence) for sentence in sentences]\n",
        "print(tokens_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8be47688-d4f5-4094-af20-3fd0a02c2ab9",
      "metadata": {
        "id": "8be47688-d4f5-4094-af20-3fd0a02c2ab9"
      },
      "outputs": [],
      "source": [
        "# pad each sentence\n",
        "\n",
        "padded_tokens_list = [list(pad_both_ends(tokens, n=N)) for tokens in tokens_list]\n",
        "print(padded_tokens_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b9b2b45-8cbb-49bf-a541-2ae51139401e",
      "metadata": {
        "id": "2b9b2b45-8cbb-49bf-a541-2ae51139401e"
      },
      "outputs": [],
      "source": [
        "# merge sentences into a single list\n",
        "\n",
        "from nltk.lm.preprocessing import flatten\n",
        "\n",
        "padded_tokens = list(flatten(padded_tokens_list))\n",
        "print(padded_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41081167-b3fb-4e23-9bc9-89a4e631aa96",
      "metadata": {
        "id": "41081167-b3fb-4e23-9bc9-89a4e631aa96"
      },
      "outputs": [],
      "source": [
        "# create n-grams\n",
        "\n",
        "grams = list(ngrams(padded_tokens, n=N))\n",
        "print(grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3966a928-13d1-43c8-a07a-b3e021a978c2",
      "metadata": {
        "id": "3966a928-13d1-43c8-a07a-b3e021a978c2"
      },
      "outputs": [],
      "source": [
        "# finding the vocabulary of ngram model\n",
        "\n",
        "vocabulary = list(set(padded_tokens))\n",
        "print(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e69245c-c669-4f14-bb79-45e6cb79ef33",
      "metadata": {
        "id": "8e69245c-c669-4f14-bb79-45e6cb79ef33"
      },
      "outputs": [],
      "source": [
        "# Language Model\n",
        "\n",
        "from nltk.lm import MLE\n",
        "\n",
        "lm = MLE(N)\n",
        "\n",
        "lm.fit([grams], vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "333a64df-b680-4265-a7c0-8cc48ce68d93",
      "metadata": {
        "id": "333a64df-b680-4265-a7c0-8cc48ce68d93"
      },
      "outputs": [],
      "source": [
        "# model vocabulary\n",
        "\n",
        "print(lm.vocab)\n",
        "print(list(lm.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a40f8110-9552-4678-a258-7c446fd9bdeb",
      "metadata": {
        "id": "a40f8110-9552-4678-a258-7c446fd9bdeb"
      },
      "outputs": [],
      "source": [
        "# check if a word exists in vocabulary\n",
        "\n",
        "lm.vocab.lookup(['text', 'and', 'sherlock'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "377ae6a9-3c8a-42ac-8139-ce051d2b260c",
      "metadata": {
        "id": "377ae6a9-3c8a-42ac-8139-ce051d2b260c"
      },
      "outputs": [],
      "source": [
        "# ngram counts\n",
        "\n",
        "# a particular bigram\n",
        "print(lm.counts[['a']]['sequence'])\n",
        "\n",
        "# all bigrams starting with a word\n",
        "lm.counts[['a']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de4895f-5913-47d4-ba0a-e973a63f6c46",
      "metadata": {
        "id": "6de4895f-5913-47d4-ba0a-e973a63f6c46"
      },
      "outputs": [],
      "source": [
        "# ngram probabilities\n",
        "\n",
        "# probability of 'sequence' occurring after 'a'\n",
        "lm.score('sequence', ['a'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5455d01d-d2b9-4db6-9091-078bbf2f6cbd",
      "metadata": {
        "id": "5455d01d-d2b9-4db6-9091-078bbf2f6cbd"
      },
      "outputs": [],
      "source": [
        "# log probability to avoid very small values\n",
        "\n",
        "lm.logscore('sequence', ['a'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "489c6b75-4326-4831-bff4-6b126f382095",
      "metadata": {
        "id": "489c6b75-4326-4831-bff4-6b126f382095"
      },
      "outputs": [],
      "source": [
        "# text generation\n",
        "\n",
        "generated_tokens = lm.generate(num_words=50, text_seed=['<s>', '<s>', '<s>'])\n",
        "generated_text = \" \".join(generated_tokens)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8045068f-0281-4e32-a5ea-52ee56df737a",
      "metadata": {
        "id": "8045068f-0281-4e32-a5ea-52ee56df737a"
      },
      "source": [
        "**Exercise 11.1** Create a character-based n-gram model using any book. Use the language model to write a short story."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4265576c-8ad0-4997-b200-45bd892ccef1",
      "metadata": {
        "id": "4265576c-8ad0-4997-b200-45bd892ccef1"
      },
      "outputs": [],
      "source": [
        "# TODO: Your Code Here\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.lm.preprocessing import pad_both_ends, flatten\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm import MLE\n",
        "\n",
        "N = 5\n",
        "\n",
        "with open('assets/sherlock.txt', 'r') as f:\n",
        "    raw_text = f.read()\n",
        "raw_text\n",
        "\n",
        "sentences = sent_tokenize(raw_text)\n",
        "tokens_list = [list(sentence) for sentence in sentences]\n",
        "\n",
        "padded_tokens_list = [list(pad_both_ends(tokens, n=N)) for tokens in tokens_list]\n",
        "padded_tokens = list(flatten(padded_tokens_list))\n",
        "\n",
        "grams = list(ngrams(padded_tokens, n=N))\n",
        "\n",
        "vocabulary = list(set(padded_tokens))\n",
        "\n",
        "lm = MLE(N)\n",
        "lm.fit([grams], vocabulary)\n",
        "\n",
        "print(list(lm.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c50eb810-9321-4b3a-95e6-3e3d4c3a9814",
      "metadata": {
        "id": "c50eb810-9321-4b3a-95e6-3e3d4c3a9814"
      },
      "outputs": [],
      "source": [
        "generated_tokens = lm.generate(num_words=500, text_seed=['<s>']*(N-1))\n",
        "generated_text = ''.join(generated_tokens)\n",
        "\n",
        "generated_text = generated_text.replace(\"<s>\", \" \")\n",
        "generated_text = generated_text.replace(\"</s>\", \" \")\n",
        "generated_text = \" \".join(generated_text.split())\n",
        "\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93b5a629-1fa3-4aa5-a9c1-8a2eaae7f6d6",
      "metadata": {
        "id": "93b5a629-1fa3-4aa5-a9c1-8a2eaae7f6d6"
      },
      "source": [
        "**Ngram Probabilities Calculation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a8e8626-eba0-44e7-b435-7f8039ff61e6",
      "metadata": {
        "id": "7a8e8626-eba0-44e7-b435-7f8039ff61e6"
      },
      "outputs": [],
      "source": [
        "text = \"To be, or not to be, that is the question\"\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c3cfc4-a294-40e2-8880-513db0e726b3",
      "metadata": {
        "id": "57c3cfc4-a294-40e2-8880-513db0e726b3"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7139686b-fc1f-4b81-a483-73cd8d285e22",
      "metadata": {
        "id": "7139686b-fc1f-4b81-a483-73cd8d285e22"
      },
      "outputs": [],
      "source": [
        "tokens = [token.lower() for token in tokens]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f58f88-a021-422d-8ee5-6b06b87952df",
      "metadata": {
        "id": "60f58f88-a021-422d-8ee5-6b06b87952df"
      },
      "outputs": [],
      "source": [
        "tokens = [token for token in tokens if token not in string.punctuation]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea3fbb88-6f68-41b6-b50c-8d896515e67e",
      "metadata": {
        "id": "ea3fbb88-6f68-41b6-b50c-8d896515e67e"
      },
      "outputs": [],
      "source": [
        "vocabulary = sorted(set(tokens))\n",
        "print(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7052e527-a9e1-427c-b6b6-2515ac55ec0c",
      "metadata": {
        "id": "7052e527-a9e1-427c-b6b6-2515ac55ec0c"
      },
      "outputs": [],
      "source": [
        "word2index = {v: i for i, v in enumerate(vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8442618c-1120-45dd-84bd-5a298bd6365f",
      "metadata": {
        "id": "8442618c-1120-45dd-84bd-5a298bd6365f"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "grams = list(ngrams(tokens, n=2))\n",
        "print(grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ebc0d0a-3238-4ac0-a795-5fc3c528ed1d",
      "metadata": {
        "id": "7ebc0d0a-3238-4ac0-a795-5fc3c528ed1d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib as mpl\n",
        "\n",
        "\n",
        "def plot_matrix_with_names(matrix, namesx, namesy):\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(matrix)\n",
        "\n",
        "    # Show all ticks and label them with the respective list entries\n",
        "    ax.set_xticks(np.arange(len(namesx)), labels=namesx)\n",
        "    ax.set_yticks(np.arange(len(namesy)), labels=namesy)\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    for i in range(len(namesy)):\n",
        "        for j in range(len(namesx)):\n",
        "            text = ax.text(j, i, matrix[i, j],\n",
        "                           ha=\"center\", va=\"center\", color=\"w\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adb0ff86-0752-4a16-b2ea-398f7c523a68",
      "metadata": {
        "id": "adb0ff86-0752-4a16-b2ea-398f7c523a68"
      },
      "outputs": [],
      "source": [
        "counts = np.zeros((len(vocabulary), len(vocabulary)), dtype=np.int32)\n",
        "\n",
        "for gram in grams:\n",
        "    previous_word = gram[0]\n",
        "    next_word = gram[1]\n",
        "\n",
        "    i = word2index[previous_word]\n",
        "    j = word2index[next_word]\n",
        "\n",
        "    counts[i][j] += 1\n",
        "\n",
        "plot_matrix_with_names(counts, vocabulary, vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15e1ba60-7f5c-4163-b5ae-5ec67f07d34e",
      "metadata": {
        "id": "15e1ba60-7f5c-4163-b5ae-5ec67f07d34e"
      },
      "outputs": [],
      "source": [
        "total_word_counts = np.sum(counts, axis=1, keepdims=True)\n",
        "\n",
        "total_word_counts[total_word_counts==0] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c50a8daf-e8bb-43cd-8a1e-c3e21a2dab34",
      "metadata": {
        "id": "c50a8daf-e8bb-43cd-8a1e-c3e21a2dab34"
      },
      "outputs": [],
      "source": [
        "probabilities = counts / total_word_counts\n",
        "probabilities = np.around(probabilities, 2)\n",
        "\n",
        "plot_matrix_with_names(probabilities, vocabulary, vocabulary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84681104-b9a2-4f24-9a09-d7b3fc9224b3",
      "metadata": {
        "id": "84681104-b9a2-4f24-9a09-d7b3fc9224b3"
      },
      "source": [
        "**Onehot Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5de1ec71-0e63-49b4-82da-f97b7b4ef577",
      "metadata": {
        "id": "5de1ec71-0e63-49b4-82da-f97b7b4ef577"
      },
      "outputs": [],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a6a7b5a-ca5f-4077-b3c3-b9e06557b8a2",
      "metadata": {
        "id": "4a6a7b5a-ca5f-4077-b3c3-b9e06557b8a2"
      },
      "outputs": [],
      "source": [
        "vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f885bf57-41ae-4576-9c5c-9ace77c3afc1",
      "metadata": {
        "id": "f885bf57-41ae-4576-9c5c-9ace77c3afc1"
      },
      "outputs": [],
      "source": [
        "word2index = {v: i for i, v in enumerate(vocabulary)}\n",
        "word2index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76443e98-1cca-40af-9f58-107634de8cfe",
      "metadata": {
        "id": "76443e98-1cca-40af-9f58-107634de8cfe"
      },
      "outputs": [],
      "source": [
        "# representing 'be'\n",
        "\n",
        "onehot_vector = np.zeros((len(vocabulary)))\n",
        "onehot_vector[0] = 1\n",
        "onehot_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c99f795-b8eb-4853-8080-8665ee9bdbdf",
      "metadata": {
        "id": "9c99f795-b8eb-4853-8080-8665ee9bdbdf"
      },
      "outputs": [],
      "source": [
        "# representing 'to'\n",
        "\n",
        "onehot_vector = np.zeros((len(vocabulary)))\n",
        "onehot_vector[7] = 1\n",
        "onehot_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33be8678-3295-4b19-a3a0-9efe0ab5faaa",
      "metadata": {
        "id": "33be8678-3295-4b19-a3a0-9efe0ab5faaa"
      },
      "outputs": [],
      "source": [
        "# general representation\n",
        "\n",
        "vectors_list = []\n",
        "for word in tokens:\n",
        "    onehot_vector = np.zeros((len(vocabulary)))\n",
        "    word_index = word2index[word]\n",
        "    onehot_vector[word_index] = 1\n",
        "    vectors_list.append(onehot_vector)\n",
        "\n",
        "vectors_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e36de1e-ca87-4435-b29f-8fd76a3a0d96",
      "metadata": {
        "id": "0e36de1e-ca87-4435-b29f-8fd76a3a0d96"
      },
      "outputs": [],
      "source": [
        "plot_matrix_with_names(np.array(vectors_list), vocabulary, tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "779e1c6d-8a0e-44dc-a59b-4be90316522e",
      "metadata": {
        "id": "779e1c6d-8a0e-44dc-a59b-4be90316522e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}